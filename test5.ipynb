{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def gaf(X):\n",
    "\n",
    "    X_normalized = X.reshape(-1, 1).flatten()\n",
    "\n",
    "    # Pairwise differences\n",
    "    X_diff = np.expand_dims(X_normalized, axis=0) - np.expand_dims(X_normalized, axis=1)\n",
    "\n",
    "    # Gramian Angular Field\n",
    "    GAF = np.cos(X_diff)\n",
    "\n",
    "    return GAF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_stock_price(stock_symbol, start_date, end_date):\n",
    "    # 使用 yf.Ticker() 建立 Ticker 對象\n",
    "    stock = yf.Ticker(stock_symbol)\n",
    "\n",
    "    # 使用 history() 方法取得歷史價格資訊\n",
    "    stock_data = stock.history(start=start_date, end=end_date)\n",
    "\n",
    "    return stock_data\n",
    "\n",
    "stock_symbol = '5871.TW'\n",
    "\n",
    "# 起始日期和結束日期\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "# 擷取股票價格資訊\n",
    "stock_price_data = fetch_stock_price(stock_symbol=stock_symbol, start_date='2012-01-02',end_date=end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price_data['do'] = stock_price_data['Open'].pct_change()\n",
    "stock_price_data['dh'] = stock_price_data['High'].pct_change()\n",
    "stock_price_data['dl'] = stock_price_data['Low'].pct_change()\n",
    "stock_price_data['dc'] = stock_price_data['Close'].pct_change()\n",
    "stock_price_data['dv'] = stock_price_data['Volume'].pct_change()\n",
    "stock_price_data['oc'] = stock_price_data['Open']-stock_price_data['Close']\n",
    "\n",
    "stock_price_data['curr_bar_state'] = np.sign(stock_price_data['oc'])\n",
    "stock_price_data = stock_price_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_forward = 10\n",
    "for i in range(1, p_forward+1):\n",
    "    stock_price_data[f'bar_state_{str(i)}'] = stock_price_data['curr_bar_state'].shift(-i-p_forward+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = stock_price_data.iloc[:,7:]\n",
    "df = stock_price_data\n",
    "# Replace infinite values with NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.loc['2023':]\n",
    "df = df.loc[:'2022']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer()\n",
    "df[['do', 'dh', 'dl', 'dc', 'dv']] = scaler.fit_transform(df[['do', 'dh', 'dl', 'dc', 'dv']])\n",
    "df_test[['do', 'dh', 'dl', 'dc', 'dv']] = scaler.fit_transform(df_test[['do', 'dh', 'dl', 'dc', 'dv']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2669 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2669/2669 [00:07<00:00, 351.75it/s]\n"
     ]
    }
   ],
   "source": [
    "window_size = 10\n",
    "\n",
    "x1_list, y1_list = [], []\n",
    "\n",
    "# Iterate over the DataFrame to create the training and testing sets\n",
    "for i in tqdm(range(len(df)-window_size+1)):\n",
    "    window = df.iloc[i:i+window_size]  # Extract the window of data\n",
    "    # print(window.T.values)\n",
    "    x1_values = window[['do', 'dh', 'dl', 'dc', 'dv']].T.values  # Adjust column names as needed\n",
    "    # print(x1_values)\n",
    "    # print(window[['bar_state_1', 'bar_state_2', 'bar_state_3', 'bar_state_4', 'bar_state_5']])\n",
    "    # y1_values = window[['bar_state_1', 'bar_state_2', 'bar_state_3', 'bar_state_4', 'bar_state_5']].iloc[0].T.values # Take the last value of 'bar_state_1' as the output\n",
    "    y1_values = window[['bar_state_1']].iloc[0].T.values\n",
    "    x1_list.append(x1_values)\n",
    "    y1_list.append(y1_values)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "x = np.array(x1_list)\n",
    "y = np.array(y1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(len(x)):\n",
    "    X_element = []\n",
    "    for j in range(len(x[i])):\n",
    "        X_element.append(gaf(x[i][j]))\n",
    "        # print(gaf(x[i][j]))\n",
    "\n",
    "    X.append(X_element)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# valid\n",
    "percentage = 20\n",
    "num_numbers = int((percentage / 100) * len(X))\n",
    "\n",
    "# Generate a list of randomly selected numbers\n",
    "valid_numbers = random.sample(range(0, len(X)), num_numbers)\n",
    "training_numbers = [num for num in range(0, len(X)) if num not in valid_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X[training_numbers]\n",
    "x_valid = X[valid_numbers]\n",
    "\n",
    "y_train = y[training_numbers]\n",
    "y_valid = y[valid_numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the basic block with skip connection\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Define the ResNet model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(5, 64, kernel_size=3, stride=1, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return torch.tanh(x)\n",
    "\n",
    "# Create the ResNet-50 model\n",
    "def resnet50():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "# Instantiate the model\n",
    "model = resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_valid, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_valid, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "dataset_train = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "dataset_valid = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train , batch_size=128, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid , batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] Training Loss: 1.0956386328 Valid Loss: 0.0736543480\n",
      "Epoch [2/500] Training Loss: 0.8983930349 Valid Loss: 0.0733987680\n",
      "Epoch [3/500] Training Loss: 0.7699970603 Valid Loss: 0.0847673863\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Windows\\Temp\\ipykernel_10576\\279883132.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\Anaconda3\\envs\\sim_search\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\Anaconda3\\envs\\sim_search\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for batch_x, batch_y in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "\n",
    "        loss = criterion(outputs, batch_y) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x_val, batch_y_val in dataloader_valid:\n",
    "            outputs_val = model(batch_x_val)\n",
    "            loss_val = criterion(outputs_val, batch_y_val)\n",
    "            val_loss.append(loss_val.item())\n",
    "\n",
    "\n",
    "        # Print statistics\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]', \n",
    "            f'Training Loss: {loss.item():.10f}',\n",
    "            f'Valid Loss: {sum(val_loss)/64:.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:00<00:00, 416.15it/s]\n"
     ]
    }
   ],
   "source": [
    "window_size = 10\n",
    "\n",
    "x2_list, y2_list = [], []\n",
    "\n",
    "# Iterate over the DataFrame to create the training and testing sets\n",
    "for i in tqdm(range(len(df_test)-window_size+1)):\n",
    "    window = df_test.iloc[i:i+window_size]  # Extract the window of data\n",
    "    # print(window.T.values)\n",
    "    x2_values = window[['do', 'dh', 'dl', 'dc', 'dv']].T.values  # Adjust column names as needed\n",
    "    # print(x1_values)\n",
    "    # print(window[['bar_state_1', 'bar_state_2', 'bar_state_3', 'bar_state_4', 'bar_state_5']])\n",
    "    # y1_values = window[['bar_state_1', 'bar_state_2', 'bar_state_3', 'bar_state_4', 'bar_state_5']].iloc[0].T.values # Take the last value of 'bar_state_1' as the output\n",
    "    y2_values = window[['bar_state_1']].iloc[0].T.values\n",
    "    x2_list.append(x2_values)\n",
    "    y2_list.append(y2_values)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "x2 = np.array(x2_list)\n",
    "y2 = np.array(y2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = []\n",
    "for i in range(len(x2)):\n",
    "    X_element = []\n",
    "    for j in range(len(x2[i])):\n",
    "        X_element.append(gaf(x2[i][j]))\n",
    "\n",
    "    X2.append(X_element)\n",
    "X2 = np.array(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tensor = torch.tensor(X2, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y2, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.sign(model(x_test_tensor)) - y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4739130434782609"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-len(torch.nonzero(torch.sum(b, dim=1)))/len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
